collected all the links ( around 2000 ) using get links.py

the file -> Loksattalinks.txt consist of all the links 

The links belong to some predefined domain like :
Enterntainment , tech , lifestyle , politics , maharashtra , etc

Since I wanted data to be organises and easy to work on I decided to Store the links in different folders .
Besides it will be impossible for one person to work on entire dataset ( when we fetch data it will be humungous ) .
So organising them based on domain will be managebale as we can ask people to work on it based on their domain interest .

To achieve this -> I have created : sort_links.py 
This script sorts and stores all the links into different text files 
Thus one can later use one script - changing reference files , to scrape data .

25 march 2025
Status : Links are sorted into different folders.

To manage folder effectively I am thinking of creating two more branches 
Branches ->
1) Data : This will act as backup for scraped data
2) Work : This is where we will word and once data is no longer needed I can delete data

Flow ( this is overall workflow , changes can be made later ): 
git branch data
git checkout data
--> Scrape and store data See next section ( Main Data )
git branch work
git checkout work
--> Annote data , train model , etc
--> Delete Main data from dataset except for links .
git add .
git commit -m "deleted main data but have backup in other branch"
git checkout main
git merge work
(Remember we have yet to make any progress this is just flashforward of project workflow )
----#---#----

Status : 



